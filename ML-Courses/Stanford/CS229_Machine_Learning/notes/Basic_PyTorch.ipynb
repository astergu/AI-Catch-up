{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook aims to give a very brief introduction to deep learning as implemented in PyTorch. Note that knowledge of PyTorch is *not* required for CS 229 (although you may use it for your final project); its use here is to demonstrate how to work with a popular modern deep learning library.\n",
        "\n",
        "First, run the cell below to import PyTorch and load the data we'll be using in this tutorial."
      ],
      "metadata": {
        "id": "0NZsyynMWIOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device('cuda')\n",
        "\n",
        "import pandas as pd\n",
        "def load_housing_data(path, y_key='median_house_value'):\n",
        "  table = torch.from_numpy(pd.read_csv(path).to_numpy()).float().to(device)\n",
        "  return table[:,:-1], table[:,-1]\n",
        "X_train, y_train = load_housing_data('sample_data/california_housing_train.csv')\n",
        "X_test, y_test = load_housing_data('sample_data/california_housing_test.csv')\n",
        "\n",
        "# normalize inputs\n",
        "x_mean, x_std = X_train.mean(0), X_train.std(0)\n",
        "X_train = (X_train - x_mean) / x_std\n",
        "X_test = (X_test - x_mean) / x_std\n",
        "# rescale outputs\n",
        "y_train = y_train / 1000\n",
        "y_test = y_test / 1000"
      ],
      "metadata": {
        "id": "LsQEqdcQf5yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic objects in PyTorch are Tensors, which are multidimensional arrays similar to NumPy arrays. In fact, much of the PyTorch function calls and indexing are the same as in NumPy."
      ],
      "metadata": {
        "id": "t8mdqITrgE9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(0.).shape, torch.tensor([0., 1.]).shape, torch.tensor([[0., 1.], [2., 3.], [4., 5.]]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0flwR2dFHnM",
        "outputId": "4e0b99b8-6098-463e-ad99-9ddf8a528461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([]), torch.Size([2]), torch.Size([3, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHVYkqAeV_0N",
        "outputId": "764d2591-751f-4eed-86a3-ccbbd44c05a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
            "tensor([ 1,  2,  0,  0,  0,  6,  7,  8,  9, 10])\n",
            "tensor([ 3,  5,  1,  1,  1, 13, 15, 17, 19, 21])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(1, 11)\n",
        "print(x)\n",
        "x[2:5] = 0\n",
        "print(x)\n",
        "print(2*x+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One difference from NumPy arrays is that tensors have an attribute, `requires_grad`, that tells PyTorch to keep track of the gradient operations. By default, this flag is false for normal tensors that you construct. But a special type of Tensor, called a Parameter, has `requires_grad = True`. Also, if you compute a function of a Tensor that requires grad, the resulting tensor will also require grad."
      ],
      "metadata": {
        "id": "VpA_irekoIDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = nn.Parameter(torch.rand(5))\n",
        "b = nn.Parameter(torch.rand(1))\n",
        "x = torch.rand(5)\n",
        "out = torch.inner(theta, x) + b\n",
        "print(theta.requires_grad, b.requires_grad, x.requires_grad, out.requires_grad)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g1vNG78pdLZ",
        "outputId": "839bb05a-55d7-40e4-aa00-4b43911a6c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True True False True\n",
            "tensor([1.8037], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below constructs a simple multi-layer perceptron (MLP):"
      ],
      "metadata": {
        "id": "kdX7q93QZR5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "hidden_units = 256\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_dim, hidden_units),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_units, hidden_units),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_units, hidden_units),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_units, 1)  # scalar output\n",
        ")\n",
        "model.to(device)  # move to GPU"
      ],
      "metadata": {
        "id": "j5RoueGtY_OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae33960b-5a3c-4583-840f-63c3ba185129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=8, out_features=256, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model created, we now construct an optimizer object that will handle updating the model's parameters:"
      ],
      "metadata": {
        "id": "GCZlbp_IZx3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "cwEU1UvVaVQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train the model! The main loop involves the following steps:\n",
        "* Sample a batch of data\n",
        "* Compute the loss on the batch of data\n",
        "* Call the optimizer's `.zero_grad()` method to clear any existing gradient information stored in the parameters.\n",
        "* Call the loss's `.backward()` method to backpropagate gradients to the parameters\n",
        "* Call the optimizer's `.step()` method to update the parameters using the new gradient information\n",
        "\n"
      ],
      "metadata": {
        "id": "TxbDVyvkarfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = X_train.shape[0]\n",
        "num_steps = 50000\n",
        "batch_size = 256\n",
        "loss_ema = None\n",
        "for step_index in range(num_steps):\n",
        "  batch_indices = torch.randint(high=n, size=[batch_size])\n",
        "  X_batch = X_train[batch_indices]\n",
        "  y_batch = y_train[batch_indices]\n",
        "  predictions = model(X_batch).squeeze(-1)  # squeeze reshapes [B, 1] -> [B]\n",
        "  loss = nn.functional.mse_loss(predictions, y_batch)\n",
        "  loss_ema = loss.item() if loss_ema is None else (0.1 * loss.item() + 0.9 * loss_ema)\n",
        "  if step_index % 1000 == 0:\n",
        "    print(f'Loss EMA at step {step_index}: {loss_ema}')\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()   # backpropagate\n",
        "  optimizer.step()  # update"
      ],
      "metadata": {
        "id": "RHOUzm2tdShh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a14062c-a240-4563-a80b-1b412fe0505a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss EMA at step 0: 56162.14453125\n",
            "Loss EMA at step 1000: 3297.1064133197115\n",
            "Loss EMA at step 2000: 2813.036880667342\n",
            "Loss EMA at step 3000: 2948.2840153701827\n",
            "Loss EMA at step 4000: 2493.5022401937804\n",
            "Loss EMA at step 5000: 2527.959669862538\n",
            "Loss EMA at step 6000: 2393.5583275584286\n",
            "Loss EMA at step 7000: 2383.5043021309884\n",
            "Loss EMA at step 8000: 2197.845718176544\n",
            "Loss EMA at step 9000: 2245.4929384119832\n",
            "Loss EMA at step 10000: 2401.8460390299865\n",
            "Loss EMA at step 11000: 2124.3930005829197\n",
            "Loss EMA at step 12000: 2082.2648568245804\n",
            "Loss EMA at step 13000: 2109.5585373673043\n",
            "Loss EMA at step 14000: 2062.9334114769695\n"
          ]
        }
      ]
    }
  ]
}