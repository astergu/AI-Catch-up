[https://www.coursera.org/learn/generative-ai-with-llms](https://www.coursera.org/learn/generative-ai-with-llms)


In this course, you will explore in detail the steps that make up a typical generative AI project lifecycle, from scoping the problem and
selecting a language model to optimizing a model for deployment and integrating into your applications. 


# Week 1

## Generative AI & Large Language Model Use Cases & Model Lifecycle

Previous generations of language models made use of an architecture called `recurrent neural neworks` or `RNNs`. `RNNs` while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks.

In 2017, after the publication of the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), from Google and the University of Toronto, the `transformer` architecture had arrived. It can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger training datasets, and cruically, it's able to learn to pay attention to the meaning of the word it's processing.